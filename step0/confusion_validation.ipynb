{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf0995f-6fbe-4741-b786-19460013c3b2",
   "metadata": {},
   "source": [
    "# The Validation Experiment Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a397535-ef77-42d2-81d9-681e4ea733ca",
   "metadata": {},
   "source": [
    "## 1.3 Implementing the confusion detection experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49354883-5e8c-4193-8d98-079a161cbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "from model_classes import InstrumentedTransformer, ModelConfig, ConfusionDetector\n",
    "from data_generation import generate_arithmetic_batch, VOCAB\n",
    "\n",
    "def run_confusion_validation():\n",
    "    \"\"\"\n",
    "    The experiment that proves (or disproves) confusion detection\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize\n",
    "    config = ModelConfig()\n",
    "    config.vocab_size = VOCAB.vocab_size  # Use actual vocab size\n",
    "    model = InstrumentedTransformer(config)\n",
    "    detector = ConfusionDetector()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    results = {\n",
    "        'phase1_addition': [],\n",
    "        'phase2a_multiplication': [],\n",
    "        'phase2b_subtraction': []\n",
    "    }\n",
    "    \n",
    "    # Phase 1: Addition Training\n",
    "    print(\"=\"*60)\n",
    "    print(\"Phase 1: Training on Addition\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for step in range(500):\n",
    "        batch = generate_arithmetic_batch(operation='add', batch_size=32)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, metadata = model(batch['input'], return_metadata=True)\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=VOCAB.pad_id)(\n",
    "            logits.view(-1, logits.size(-1)), \n",
    "            batch['target'].view(-1)\n",
    "        )\n",
    "        \n",
    "        # Compute confusion metrics\n",
    "        metrics = detector.compute_all_metrics(model, batch, metadata, loss)\n",
    "        results['phase1_addition'].append(metrics)\n",
    "        \n",
    "        # Train\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step:3d} | Loss: {loss.item():.4f} | \"\n",
    "                  f\"Attn Entropy: {metrics['attention_entropy']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n✓ Addition baseline established\")\n",
    "    print(f\"  Final loss: {results['phase1_addition'][-1]['loss']:.4f}\")\n",
    "    \n",
    "    # Phase 2a: Multiplication (NO training, just measure confusion)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Phase 2a: Testing on MULTIPLICATION (Architectural Challenge)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model.eval()  # Important: eval mode for testing\n",
    "    with torch.no_grad():\n",
    "        for step in range(200):\n",
    "            batch = generate_arithmetic_batch(operation='multiply', batch_size=32)\n",
    "            \n",
    "            logits, metadata = model(batch['input'], return_metadata=True)\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=VOCAB.pad_id)(\n",
    "                logits.view(-1, logits.size(-1)), \n",
    "                batch['target'].view(-1)\n",
    "            )\n",
    "            \n",
    "            metrics = detector.compute_all_metrics(model, batch, metadata, loss)\n",
    "            results['phase2a_multiplication'].append(metrics)\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                print(f\"Step {step:3d} | Loss: {loss.item():.4f} | \"\n",
    "                      f\"Attn Entropy: {metrics['attention_entropy']:.4f}\")\n",
    "    \n",
    "    # Phase 2b: Subtraction (NO training, just measure confusion)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Phase 2b: Testing on SUBTRACTION (Parametric Challenge)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in range(200):\n",
    "            batch = generate_arithmetic_batch(operation='subtract', batch_size=32)\n",
    "            \n",
    "            logits, metadata = model(batch['input'], return_metadata=True)\n",
    "            loss = nn.CrossEntropyLoss(ignore_index=VOCAB.pad_id)(\n",
    "                logits.view(-1, logits.size(-1)), \n",
    "                batch['target'].view(-1)\n",
    "            )\n",
    "            \n",
    "            metrics = detector.compute_all_metrics(model, batch, metadata, loss)\n",
    "            results['phase2b_subtraction'].append(metrics)\n",
    "            \n",
    "            if step % 50 == 0:\n",
    "                print(f\"Step {step:3d} | Loss: {loss.item():.4f} | \"\n",
    "                      f\"Attn Entropy: {metrics['attention_entropy']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Experiment Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad25d89-1138-4a3a-945c-4c818961cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Phase 1: Training on Addition\n",
      "============================================================\n",
      "Step   0 | Loss: 3.1543 | Attn Entropy: 2.1979\n",
      "Step 100 | Loss: 1.2235 | Attn Entropy: 1.7412\n",
      "Step 200 | Loss: 1.0045 | Attn Entropy: 1.6309\n",
      "Step 300 | Loss: 0.8446 | Attn Entropy: 1.5665\n",
      "Step 400 | Loss: 0.8942 | Attn Entropy: 1.5330\n",
      "\n",
      "✓ Addition baseline established\n",
      "  Final loss: 0.7900\n",
      "\n",
      "============================================================\n",
      "Phase 2a: Testing on MULTIPLICATION (Architectural Challenge)\n",
      "============================================================\n",
      "Step   0 | Loss: 2.9072 | Attn Entropy: 1.4525\n",
      "Step  50 | Loss: 2.7322 | Attn Entropy: 1.4705\n",
      "Step 100 | Loss: 3.2209 | Attn Entropy: 1.4417\n",
      "Step 150 | Loss: 3.1024 | Attn Entropy: 1.4484\n",
      "\n",
      "============================================================\n",
      "Phase 2b: Testing on SUBTRACTION (Parametric Challenge)\n",
      "============================================================\n",
      "Step   0 | Loss: 4.0218 | Attn Entropy: 1.4939\n",
      "Step  50 | Loss: 3.8147 | Attn Entropy: 1.4842\n",
      "Step 100 | Loss: 4.8311 | Attn Entropy: 1.5326\n",
      "Step 150 | Loss: 3.8786 | Attn Entropy: 1.5384\n",
      "\n",
      "============================================================\n",
      "Experiment Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import importlib\n",
    "import model_classes\n",
    "importlib.reload(model_classes)\n",
    "from model_classes import InstrumentedTransformer, ModelConfig, ConfusionDetector\n",
    "\n",
    "results = run_confusion_validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
