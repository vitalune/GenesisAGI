{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d1de48-cee7-4958-bc22-c06ea525e920",
   "metadata": {},
   "source": [
    "# The Validation Experiment Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7350d96-98de-4133-b468-a9bd3218b19d",
   "metadata": {},
   "source": [
    "## 1.1 Building the minimal transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d7d9f9-d0dd-4441-b3e0-46fbec1fdfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d62ea926-d6ab-447a-ace0-7fa1ac40862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "\n",
    "# keeping everything tiny, for now\n",
    "class ModelConfig:\n",
    "    d_model: int = 32\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 2\n",
    "    d_ff: int = 64\n",
    "    vocab_size: int = 20\n",
    "    max_seq_len: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cdb7c85-ef82-4797-b125-656e27f8f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyAttentionHead(nn.Module):\n",
    "    \"\"\"single attention head with instrumentation\"\"\"\n",
    "    def __init__(self, d_model, head_dim):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_proj = nn.Linear(d_model, head_dim)\n",
    "        self.k_proj = nn.Linear(d_model, head_dim)\n",
    "        self.v_proj = nn.Linear(d_model, head_dim)\n",
    "        \n",
    "        # instrumentation\n",
    "        self.attention_patterns = []\n",
    "        self.activation_magnitudes = []\n",
    "        \n",
    "    def forward(self, x, return_metadata=False):\n",
    "        batch, seq, _ = x.shape\n",
    "        \n",
    "        Q = self.q_proj(x)  # (batch, seq, head_dim)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        # attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # track attention patterns\n",
    "        if return_metadata:\n",
    "            self.attention_patterns.append(attn_weights.detach())\n",
    "            self.activation_magnitudes.append(torch.norm(V, dim=-1).detach())\n",
    "        \n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        if return_metadata:\n",
    "            return output, {\n",
    "                'attn_weights': attn_weights,\n",
    "                'q_norm': torch.norm(Q, dim=-1),\n",
    "                'k_norm': torch.norm(K, dim=-1),\n",
    "                'v_norm': torch.norm(V, dim=-1),\n",
    "                'attn_entropy': self._compute_entropy(attn_weights)\n",
    "            }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _compute_entropy(self, probs):\n",
    "        # attention entropy - high = confused/uncertain\n",
    "        return -torch.sum(probs * torch.log(probs + 1e-9), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeecef20-76bc-400c-8f11-cfdb076f1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstrumentedTransformer(nn.Module):\n",
    "    \"\"\"tiny transformer with extensive instrumentation\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(config.max_seq_len, config.d_model))\n",
    "\n",
    "        self.output = nn.Linear(config.d_model, config.vocab_size)\n",
    "\n",
    "        # instrumentation storage\n",
    "        self.layer_activations = []\n",
    "        self.gradient_flows = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def forward(self, x, return_metadata=False):\n",
    "        # embed\n",
    "        x = self_embedding(x) + self.pos_encoding[:x.size(1)]\n",
    "\n",
    "        metadata = {'layers': []}\n",
    "\n",
    "        # layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if return_metadata:\n",
    "                x, layer_meta = layer(x, return_metadata=True)\n",
    "                metadata['layers'].append(layer_meta)\n",
    "            else:\n",
    "                x = layers(x)\n",
    "\n",
    "        # output\n",
    "        logits = self.outputs(x)\n",
    "\n",
    "        if return_metadata:\n",
    "            return logits, metadata\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25841647-6715-4020-8b20-b89c664d8129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([\n",
    "            TinyAttentionHead(config.d_model, config.d_model // config.n_heads)\n",
    "            for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.head_projection = nn.Linear(config.d_model, config.d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.d_ff, config.d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(config.d_model)\n",
    "        self.norm2 = nn.LayerNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x, return_metadata=False):\n",
    "        # multi head attention\n",
    "        if return_metadata:\n",
    "            head_outputs = []\n",
    "            head_metadata = []\n",
    "            for head in self.heads:\n",
    "                out, meta = head(x, return_metadata=True)\n",
    "                head_outputs.append(out)\n",
    "                head_metadata.append(meta)\n",
    "\n",
    "            attn_out = torch.cat(head_outputs, dim=-1)\n",
    "            attn_out = self.head_projection(attn_out)\n",
    "            x = self.norm1(x + attn_out)\n",
    "\n",
    "            ff_out = self.ff(x)\n",
    "            x = self.norm2(x + ff_out)\n",
    "\n",
    "            return x, {\n",
    "                'heads': head_metadata,\n",
    "                'ff_activation': torch.norm(ff_out, dim=-1),\n",
    "                'residual_norm': torch.norm(x, dim=-1)\n",
    "            }\n",
    "        else:\n",
    "            # standard forward\n",
    "            head_outputs = [head(x) for head in self.heads]\n",
    "            attn_out = torch.cat(head_outputs, dim=-1)\n",
    "            attn_out = self.head_projection(attn_out)\n",
    "            x = self.norm1(x + attn_out)\n",
    "            x = self.norm2(x + self.ff(x))\n",
    "            return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
